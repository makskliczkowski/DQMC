{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b270d6-b6c0-45e6-9ad1-442a3e45df74",
   "metadata": {},
   "source": [
    "This is the notebook that includes the procedures for machine learning to be used in the Determinant Quantum Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a5097-73e7-46a3-9068-b2a15931dfef",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING IMPORTS\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e70fb6-cf9d-44e1-b1d5-8e4440588f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38897a45-7ab1-424a-9d3c-79112904a5f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13589652021477057306\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5060693856\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14040507155647740136\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization, Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import MeanAbsoluteError, MeanAbsolutePercentageError, MeanSquaredError\n",
    "from keras.callbacks import History\n",
    "from keras import callbacks\n",
    "from keras import losses\n",
    "from keras import Input, Model\n",
    "from keras import regularizers\n",
    "from keras import initializers, optimizers\n",
    "from plot_keras_history import plot_history\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, Rescaling\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43965195-37ed-4bba-804f-b37c8eb43390",
   "metadata": {},
   "source": [
    "#### DEFINITIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84e7c744-8121-4950-bb41-229ed56d32bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results folder :  D:\\Uni\\SEMESTERS\\PRACE\\CONDENSED_GROUP_CLOUD_UNI\\DQMC\\DQMC\n",
      "D:\\Uni\\SEMESTERS\\PRACE\\CONDENSED_GROUP_CLOUD_UNI\\DQMC\\DQMC\\Python\\ml\n"
     ]
    }
   ],
   "source": [
    "markers = itertools.cycle(['o','s','v', ',', '+', '.', 'o', '*'])\n",
    "colors = itertools.cycle(sns.color_palette())\n",
    "kPSep = os.sep\n",
    "\n",
    "resultsFolder = 'D:' + kPSep + 'Uni' + kPSep + 'SEMESTERS' + kPSep + 'PRACE' + kPSep + 'CONDENSED_GROUP_CLOUD_UNI' + kPSep + 'DQMC' + kPSep + 'DQMC'\n",
    "print(\"results folder : \", resultsFolder)\n",
    "directory_path = os.getcwd()\n",
    "print(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31204c-eeb2-441f-80da-e21ae0819d57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb8ebef-6cce-4128-bfd4-77aba6843b6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d391e244-a0e4-40cc-a3f0-9e7129b27171",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "------------------\n",
    "# OTHERS\n",
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7a41b-220c-4558-a2b5-2f44244b3283",
   "metadata": {},
   "source": [
    "Check if the directories from the list are existing and if not create them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b7614dd-b7db-4158-8d6d-00cd7f401d95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createFolder(directories):\n",
    "    for folder in directories:\n",
    "        try:\n",
    "            if not os.path.isdir(folder):\n",
    "                os.makedirs(folder)\n",
    "                print(\"Created a directory : \", folder)\n",
    "        except OSError:\n",
    "            print(\"Creation of the directory %s failed\" % folder)      \n",
    "# Guard against race condition\n",
    "        except OSError as exc: \n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c30ebb-3823-4846-bb1c-5cf2b6d28f35",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CLASSES CONTAINING ALL INFO ABOUT MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e37e5c-413a-4bc3-bc38-9bb06c4f1ddf",
   "metadata": {
    "tags": []
   },
   "source": [
    "The HubbardQR model that directly corresponds to the model in the C++ simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14377e16-05d3-4192-a5a2-5ef8aea8c487",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HubbardDQMC:\n",
    "    N = 1\n",
    "    M = 1\n",
    "    def __init__(self, M, M_0, U, mu, beta, lattice_type, Lx, Ly, Lz = 1, dim = 2):\n",
    "        self.M = M\n",
    "        self.M_0 = M_0\n",
    "        self.dtau = beta/M\n",
    "        self.lambd = math.acosh(math.exp((abs(U) * self.dtau) * 0.5));\n",
    "        # physical\n",
    "        self.beta = beta\n",
    "        self.T = 1.0/beta\n",
    "        self.U = U\n",
    "        self.mu = mu\n",
    "        # lattice\n",
    "        self.lat_type = lattice_type\n",
    "        self.dim = dim\n",
    "        self.Lx = Lx\n",
    "        self.Ly = Ly\n",
    "        self.Lz = Lz\n",
    "        self.N = Lx*Ly*Lz\n",
    "        self.directory = \"results\" + kPSep \n",
    "        \n",
    "    def getInfo(self):\n",
    "        return \"M=\" + str(self.M) + \",M0=\" + str(self.M_0) + \\\n",
    "\t\t\",dtau=\" + str(self.dtau) + \",Lx=\" + str(self.Lx) + \\\n",
    "\t\t\",Ly=\" + str(self.Ly) + \",Lz=\" + str(self.Lz) + \\\n",
    "\t\t\",beta=\" + str(self.beta) + \",U=\" + str(self.U) + \\\n",
    "\t\t\",mu=\" + str(self.mu);\n",
    "    \n",
    "    def getDirectory(self):\n",
    "        LxLyLz = \"Lx=\" + str(self.Lx) + \",Ly=\" + str(self.Ly) + \",Lz=\" + str(self.Lz)        \n",
    "        return self.directory + self.lat_type + kPSep + str(self.dim) + \"D\" + kPSep + LxLyLz + kPSep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db26ecd4-0f88-43d5-ae4d-5526b8d7b7ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------------\n",
    "# IMAGES\n",
    "______________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7bbba1a-7519-4948-88b1-f8e2ef1b8046",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8132e7c-1213-4f77-8de4-5f37e86b7620",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CREATE IMAGES FROM CONFIGURATIONS SAVED TO A FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd9ef3d-736c-4963-9bf7-8032537e0794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conditions\n",
    "config_cond = lambda x: x.endswith(\".dat\") and (x.startswith(\"neg_\") or x.startswith(\"pos_\"))\n",
    "\n",
    "# functions\n",
    "def imgFromConfig(directory,savedir=\".\"+kPSep, lines=False, cond = lambda x : x, deleteFile = False):\n",
    "    createFolder([savedir])\n",
    "    for filename in filter(cond, os.listdir(directory)):\n",
    "        # read data\n",
    "        imArray = np.loadtxt(directory + filename)\n",
    "        # check if we have only one row\n",
    "        if imArray.ndim == 1:\n",
    "            imArray = np.array([ar])\n",
    "        # plot with axes\n",
    "        if lines:\n",
    "            x, y = np.linspace(0, len(ar[0]) - 1, len(ar[0])), np.linspace(0, len(ar) - 1, len(ar))\n",
    "            plt.gca().set_xticks(x)\n",
    "            plt.gca().set_xticklabels([str(i) if i % 10 == 0 else '' for i in x], rotation=90)\n",
    "            plt.gca().set_yticks(y)\n",
    "            plt.gca().set_yticklabels([str(i) if i % 10 == 0 else '' for i in y], rotation=45)\n",
    "            plt.imshow(imArray, interpolation='none')\n",
    "            plt.savefig(saveFolder + filename[:-4] + '.png')\n",
    "        else:\n",
    "            image = Image.fromarray(np.uint8(imArray * 255), \"L\") # .convert('RGB').resize(max_size, resample=Image.BOX)\n",
    "            image.save(savedir + filename[:-4] + '.png')\n",
    "        if deleteFile:\n",
    "            os.remove(directory + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817cf87b-d4c2-4d42-8cf2-9f82e6e715c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ef61f2d-c825-4462-a23e-a5a2831a2ea7",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772959e9-368f-4746-9568-1a9a833f0f02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "--------\n",
    "# HELPERS\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d830f5-b74a-415f-a0c2-43a2caa1d39b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SHUFFLE TO FILES TO DIFFERENT DIRECTORIES FOR TEST, VALIDATION AND TRAINING\n",
    "Here we will create the directories that will contain the filles according to specified category and will be shuffled to val, test and val folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb45ed14-fc79-467b-8d7a-15f933bd49d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, VALIDATION, TESTING, train_siz, val_siz):\n",
    "    files = []\n",
    "    # list files\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        file = SOURCE + kPSep + filename\n",
    "        if os.path.getsize(file) > 0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \" is zero length, so ignoring this bud.\")\n",
    "\n",
    "    training_length = int(len(files) * train_siz)\n",
    "    validation_length = int(len(files) * val_siz)\n",
    "    testing_length = int(len(files) - training_length - validation_length)\n",
    "\n",
    "    print('SOURCE: ', SOURCE, '\\n TRAINING', TRAINING, '\\n VALIDATION', VALIDATION, '\\n ', len(files))\n",
    "    print('training_length:', training_length)\n",
    "    print('validation_length:', validation_length)\n",
    "    print('testing_length:', testing_length)\n",
    "\n",
    "    shuffled_set = random.sample(files, len(files))\n",
    "    training_set = shuffled_set[0:training_length]\n",
    "    validation_set = shuffled_set[training_length:(training_length + validation_length)]\n",
    "    testing_set = shuffled_set[training_length + validation_length:-1]\n",
    "\n",
    "    # TRAIN\n",
    "    for filename in training_set:\n",
    "        this_file = SOURCE + kPSep + filename\n",
    "        destination = TRAINING + kPSep + filename\n",
    "        Path(this_file).rename(destination)\n",
    "\n",
    "    # VAL\n",
    "    for filename in validation_set:\n",
    "        this_file = SOURCE + kPSep + filename\n",
    "        destination = VALIDATION + kPSep + filename\n",
    "        Path(this_file).rename(destination)\n",
    "    # TEST\n",
    "    for filename in testing_set:\n",
    "        this_file = SOURCE + kPSep + filename\n",
    "        destination = TESTING + kPSep + filename\n",
    "        Path(this_file).rename(destination)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd9fd79f-421f-43c2-b3a0-8b8706c8fca9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shuffleToTestValidTrain(directory, train_size, val_size, classes = [''], removeFolders = False):\n",
    "    # path to the source files\n",
    "    source_path = [directory + a for a in classes]\n",
    "    # set TRAIN VALIDATION AND TEST FOLDERS\n",
    "    TRAIN_VAL_TEST = ['train', 'val', 'testing']\n",
    "    TRAIN_VAL_TEST = [directory + a + kPSep for a in TRAIN_VAL_TEST]\n",
    "    createFolder(TRAIN_VAL_TEST)\n",
    "    # set folders with classes\n",
    "    train_path = [TRAIN_VAL_TEST[0] + a  for a in classes]\n",
    "    val_path = [TRAIN_VAL_TEST[1] + a  for a in classes]\n",
    "    test_path = [TRAIN_VAL_TEST[2] + a  for a in classes]\n",
    "    createFolder(train_path + val_path + test_path)  \n",
    "    # TRY TO SPLIT FILES\n",
    "    print('\\tSPLITTING DATA')\n",
    "    for source, train_dir_path, val_dir_path, test_dir_path in zip(source_path, train_path, val_path, test_path):\n",
    "        split_data(source, train_dir_path, val_dir_path, test_dir_path, train_size, val_size)\n",
    "    if removeFolders:\n",
    "        for folder in source_path:\n",
    "            os.rmdir(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48c135-584a-41ce-a5cf-e333778f231c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "-------------------\n",
    "# NETWORKS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c546fb5-c521-46c0-b4c4-4200f28faa96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---------\n",
    "### 2D CONVOLUTION\n",
    "--------\n",
    "Below the network for 2d convolion is created, the files are taken from directories, that are split up before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aaa2550-5eaa-4e3a-9308-5607fd01cb08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createModel2D_CONV(directory, width, height, epo=4, batch=10,categories=[], color = \"grayscale\"):\n",
    "    DATASET_LOCATION = directory\n",
    "    BATCH_SIZE = batch\n",
    "    IMAGE_SIZE = (width, height)\n",
    "    SHAPE = (width, height, 1)\n",
    "    EPOCHS = epo\n",
    "    train = directory + \"train\" + kPSep\n",
    "    val = directory + \"val\"+kPSep\n",
    "    test = directory + \"testing\" + kPSep\n",
    "\n",
    "    \n",
    "    train_dataset =  tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory = train,\n",
    "        class_names = categories,\n",
    "        color_mode = color,\n",
    "        validation_split = 0,\n",
    "        seed=1337,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory = val,\n",
    "        class_names = categories,\n",
    "        color_mode = color,\n",
    "        validation_split = 0,\n",
    "        seed=1337,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory = test,\n",
    "        class_names = categories,\n",
    "        color_mode = color,\n",
    "        validation_split = 0,\n",
    "        seed=1337,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "     \n",
    "# NOW WE MAKE NETWORK\n",
    "    inputs = Input(SHAPE)\n",
    "    #x = data_augmentation(inputs)\n",
    "    #x = Rescaling(1./255)(inputs)\n",
    "# first layer\n",
    "    x = layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = layers.MaxPooling2D(2, 2, padding=\"valid\")(x)\n",
    "# second layer\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D(2, 2, padding=\"valid\")(x)\n",
    "# third layer\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu',kernel_initializer=initializers.random_normal(), padding=\"valid\",bias_initializer=initializers.zeros())(x)\n",
    "    x = layers.MaxPooling2D(2, 2, padding=\"valid\")(x)\n",
    "# fourth layer\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu',kernel_initializer=initializers.random_normal(), padding=\"valid\",bias_initializer=initializers.zeros())(x)\n",
    "    #x = layers.BatchNormalization()(x)\n",
    "# hidden\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)    \n",
    "    outputs = (layers.Dense(units=1, activation=\"sigmoid\"))(x)\n",
    "    model = Model(inputs, outputs, name=\"2Dcnn\")\n",
    "# Define the model.\n",
    "    model.summary()\n",
    "# Compile model.\n",
    "    initial_learning_rate = 0.0001\n",
    "    lr_schedule = tf.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "# Define callbacks.\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\"2d_image_classification.h5\", save_best_only=True)\n",
    "    early_stopping_cb = callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "    )\n",
    "    #plot_results(history, save_dir=directory)\n",
    "    #keras.utils.plot_model(model, show_shapes=True)\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    plot_history(history, path=directory + \"test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ba88e-b6b3-4dfd-a3e4-9de676a48410",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------\n",
    "### AUTOENCODERS\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633a301-d07e-4797-baa5-20ac02770b33",
   "metadata": {
    "tags": []
   },
   "source": [
    "The autoencoder class, given the inner dimension and outside shape it provides the autoencoding with call method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa64ed14-7eb9-4893-ae8c-08d93972eb76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim, shape):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([layers.Dense(latent_dim, activation='relu')])\n",
    "        self.decoder = tf.keras.Sequential([layers.Dense(shape[0] * shape[1])])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98a29a00-450f-4535-9511-daf096f1723d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Denoise(Model):\n",
    "    def __init__(self, shape):\n",
    "        super(Denoise, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "          layers.Input(shape=shape),\n",
    "          layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "          layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "          layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "          layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "          layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6dc5b0-84bc-46f5-af4f-9bd1d13c3ed2",
   "metadata": {},
   "source": [
    "-------\n",
    "### AUTOENCODING FROM DATA\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b43ceb3e-816c-4414-9da8-8085e0c0050f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fileAutoencode(directory, model, latent_dim, epo, trainsize, testsize = 5,x = 1, y = 1):\n",
    "    # take the folder\n",
    "    Lx = model.Lx\n",
    "    Ly = model.Ly\n",
    "    \n",
    "    folder = directory + kPSep + model.getDirectory() + \"greens\" + kPSep + model.getInfo() + kPSep + \"times\" + kPSep\n",
    "    for i in range(trainsize + testsize):\n",
    "        readTimeDisplacedGreens(folder, model.M, str(i + 2)+ \"-\", Lx, Ly)\n",
    "    \n",
    "    folder = folder + \"posDiff\" + kPSep + str((x, y)) + kPSep\n",
    "    filenum = len(os.listdir(folder))\n",
    "    data_train = []\n",
    "    data_test = []\n",
    "    counter = 0\n",
    "\n",
    "    # read files\n",
    "    for filename in filter(lambda x: x.endswith('.dat'), os.listdir(folder)):\n",
    "        tmp = pd.read_csv(folder + filename,header = None, names = ['M', 'val', 'err'], sep = '\\t')\n",
    "        if counter > filenum:\n",
    "            break\n",
    "        elif counter < trainsize: \n",
    "            data_train.append( np.array(tmp['val']))\n",
    "        elif trainsize <= counter < trainsize + testsize:\n",
    "            data_test.append( np.array(tmp['val']))\n",
    "        else:\n",
    "            break\n",
    "        counter+=1\n",
    "        print(data_test)\n",
    "    autoencoder = Autoencoder(latent_dim = latent_dim, shape = (model2.M,1))\n",
    "    autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "    autoencoder.fit((data_train),\n",
    "                epochs=epo,\n",
    "                shuffle=True,\n",
    "                validation_data=data_test,verbose=2)\n",
    "    autoencoder.save(folder + \"model.h5\")\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00ea8ff8-7d2b-42e6-965b-38962ae9ac09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compareAutoencoder_Average(directory, model, latent_dim, epo, trainsize, x, y, testsize = 5, avNum = 5):\n",
    "    Lx = model.Lx\n",
    "    Ly = model.Ly\n",
    "    network = fileAutoencode(directory, model, latent_dim, epo, trainsize, testsize, x ,y)\n",
    "\n",
    "    folder = directory + kPSep + model.getDirectory() + \"greens\" + kPSep + model.getInfo() + kPSep + \"times\" + kPSep + \"posDiff\" + kPSep + str((x, y)) + kPSep\n",
    "    \n",
    "    # read one file to test\n",
    "    tmp = pd.read_csv(folder + random.choice(os.listdir(folder)),header = None, names = ['M', 'val', 'err'], index_col = 0, sep = '\\t')\n",
    "    tester = tf.convert_to_tensor(tmp['val'])\n",
    "    for i in range(avNum - 1):\n",
    "        tmp += pd.read_csv(folder + random.choice(os.listdir(folder)),header = None, names = ['M', 'val', 'err'], index_col = 0, sep = '\\t')\n",
    "    tmp /= avNum\n",
    "    \n",
    "    # testing\n",
    "    images = network.predict([tester])\n",
    "\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.plot(images[0], label= \"after_encoder\")\n",
    "    plt.plot(tmp['val'], label = \"before_encoder\")\n",
    "    plt.xlabel('M/d$\\tau$')\n",
    "    plt.ylabel('G($\\tau$,' + str(x) + ',' + str(y) + ')')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36adf9b-313d-44c2-9a27-b7088ba19243",
   "metadata": {
    "tags": []
   },
   "source": [
    "-------------------------------------\n",
    "# PREPARATION OF CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17737d5a-9f62-4b4e-bc33-37d862b9a3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepareSignConfigs(model,epo=4, batch=10, main_directory=\"./\", deleteFiles = False): \n",
    "    config_dir = main_directory + model.getDirectory() + \"configurations\" + kPSep\n",
    "    neg_dir = config_dir + \"negative\" + kPSep\n",
    "    pos_dir = config_dir + \"positive\" + kPSep\n",
    "    images_dir = config_dir + \"images\" + kPSep\n",
    "    # make images\n",
    "    imgFromConfig(neg_dir,images_dir + \"negative\" + kPSep, False, config_cond, deleteFiles)\n",
    "    imgFromConfig(pos_dir,images_dir + \"positive\" + kPSep, False, config_cond, deleteFiles)\n",
    "    print(\"\\tIMAGES DO EXIST NOW\")\n",
    "    shuffleToTestValidTrain(images_dir, train_size = 0.6, val_size = 0.3, classes = [\"negative\", \"positive\"], removeFolders=deleteFiles)\n",
    "    print(\"\\t'ND THEM SHUFFL'D\")\n",
    "    createModel2D_CONV(images_dir,epo=epo, batch=batch, width=model.N,height=model.M, categories=[\"negative\", \"positive\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f287451-2f9d-4384-9664-ded41797db8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------------------\n",
    "# GREEN FUNCTIONS\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cacc273-c2af-4772-9a66-f791fa714401",
   "metadata": {
    "tags": []
   },
   "source": [
    "### READ UNEQUAL TIME GREENS\n",
    "\n",
    "-------------\n",
    "\n",
    "\n",
    "The function below allows the user to read the file that is saved in a triangular form starting (0,0) -> (Lx/2, 0) -> (1,1) -> (Lx/2, 1) -> (Lx/2, Ly/2) and each point has all the times printed as well $\\tau \\in \\{0,...,M-1\\}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bce182f3-34c5-4264-a480-e798a7c2dfc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cond_down = lambda x : \"downgreens\" in x\n",
    "cond_up = lambda x : \"upgreens\" in x\n",
    "def parseG_elems(line, nextline = \"\"):\n",
    "    positions = \"G(nx,ny,ti)\"\n",
    "    # looking if a given line starts with position Green\n",
    "    if line.lstrip().startswith(positions):\n",
    "        tmp = nextline.split(\"\\t\")\n",
    "        # providing next line as the positions are specified in it\n",
    "        return (\"r\",int(tmp[1]),int(tmp[3]))\n",
    "    # else we know there is data\n",
    "    else:\n",
    "        tmp = line.split(\"\\t\")\n",
    "        # return $\\tau$, x, y\n",
    "        return (int(tmp[0]), float(tmp[1]), float(tmp[3]))\n",
    "\n",
    "def readTimeDisplacedGreens(directory, M, number : str, Lx = 1, Ly = 1):\n",
    "    pos_df = pd.DataFrame()\n",
    "    pos_err_df =  pd.DataFrame()\n",
    "    neg_df =  pd.DataFrame()\n",
    "    neg_err_df =  pd.DataFrame()\n",
    "    # create directories for each position difference\n",
    "    posDir = directory + \"posDiff\"\n",
    "    createFolder([posDir])\n",
    "    for i in range(int(Lx/2) + 1):\n",
    "        for j in range(i, int(Ly/2) + 1):\n",
    "            createFolder([posDir + kPSep + str((i,j))])\n",
    "    # check all filenames and put them in separate directory\n",
    "    for filename in filter(lambda x: x.startswith(number) and x.endswith('.dat'), os.listdir(directory)):\n",
    "        with open(directory + filename, 'r') as f:\n",
    "            # skip header\n",
    "            for _ in range(33):\n",
    "                next(f)\n",
    "            # read lines\n",
    "            for line in f:\n",
    "                (dec,x,y) = parseG_elems(line, next(f))\n",
    "                if dec == 'r':\n",
    "                    tmp_col = (x,y)\n",
    "                    tmp_arr = []\n",
    "                    tmp_arr_err = []\n",
    "                    \n",
    "                    # open file to write\n",
    "                    writeF = open(posDir + kPSep + str(tmp_col) + kPSep + filename, \"w\")\n",
    "                    \n",
    "                    # save all the times\n",
    "                    for i in range(M):\n",
    "                        (tau, val, err) = parseG_elems(next(f))\n",
    "                        # write to file\n",
    "                        writeF.write(str(tau) + \"\\t\" + str(val) + '\\t' + str(err) + '\\n')\n",
    "                        tmp_arr.append(val)\n",
    "                        tmp_arr_err.append(err)\n",
    "                    \n",
    "                    # save the dataframe\n",
    "                    if \"up\" in filename:\n",
    "                        pos_df[str(tmp_col)] = np.array(tmp_arr)\n",
    "                        pos_err_df[str(tmp_col)] = np.array(tmp_arr_err)\n",
    "                    else:\n",
    "                        neg_df[str(tmp_col)] = np.array(tmp_arr)\n",
    "                        neg_err_df[str(tmp_col)] = np.array(tmp_arr_err)\n",
    "                    writeF.close()\n",
    "                # the file is not standardly parsed ;c\n",
    "                else:\n",
    "                    print(\"BADLY PARSED FILE, skip it!\")\n",
    "                    break\n",
    "                \n",
    "        f.close()\n",
    "    return (pos_df, pos_err_df, neg_df, neg_err_df)\n",
    "#a = readTimeDisplacedGreens(resultsFolder+\"\\\\results\\\\square\\\\2D\\\\Lx=8,Ly=8,Lz=1\\\\greens\\\\M=40,M0=8,dtau=0.05,Lx=8,Ly=8,Lz=1,beta=2.0,U=8.0,mu=0.0\\\\times\\\\\", 40, \"5-\", Lx = 8, Ly = 8)\n",
    "#a[0], a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814df5b-cef6-40dd-b151-a1121e62e10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691fbc4-50c7-4835-97da-a9506542fc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb86b7-5ea5-46d3-b263-6f215474121f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2726a63-583c-49ef-9021-6290be89d9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2c163f-012a-4cf8-9f16-f3251cbd9091",
   "metadata": {},
   "source": [
    "---------\n",
    "# TESTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c18eff98-2e95-404f-b9c0-40c5bda5ef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M=40,M0=8,dtau=0.05,Lx=8,Ly=8,Lz=1,beta=2.0,U=8.0,mu=0.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = 6\n",
    "Lx = 8\n",
    "Ly = 8\n",
    "mu = -1\n",
    "beta = 8\n",
    "lattice = \"square\"\n",
    "# --- \n",
    "M = 80\n",
    "M_0 = 8\n",
    "\n",
    "\n",
    "model = HubbardDQMC(M, M_0, U, mu, beta, lattice, Lx, Ly)\n",
    "model2 = HubbardDQMC(40, 8, 8.00, 0.00, 2.00, lattice, Lx, Ly)\n",
    "model2.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d960fc-9c98-4fea-b60f-554bf364519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepareSignConfigs(model,epo=50, batch=30, main_directory=resultsFolder, deleteFiles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838c0a5-1eac-4071-9c91-82de4a3016a5",
   "metadata": {},
   "source": [
    "### AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ef3a1b3f-c689-42f8-ab9c-32e4d7232f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[array([ 0.14673085,  0.11483079,  0.08847901,  0.06399239,  0.0450492 ,\n",
      "        0.0302768 ,  0.02075729,  0.01226065,  0.00764068,  0.00429023,\n",
      "        0.00391737,  0.00088302, -0.00332897, -0.00571529, -0.00929633,\n",
      "       -0.00915768, -0.01041359, -0.01160556, -0.01153638, -0.01094613,\n",
      "       -0.01024232, -0.00969038, -0.00983326, -0.00982383, -0.01046546,\n",
      "       -0.01212396, -0.01176356, -0.01008603, -0.00990458, -0.01413108,\n",
      "       -0.01913224, -0.02181008, -0.02788523, -0.03386926, -0.04119792,\n",
      "       -0.04724395, -0.05631176, -0.06821842, -0.08563838, -0.11250637])]\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>,)\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\maxgr\\AppData\\Local\\Temp/ipykernel_16704/1981640804.py:9 call  *\n        encoded = self.encoder(x)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:389 call\n        outputs = layer(inputs, **kwargs)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1008 __call__\n        self._maybe_build(inputs)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:1179 build\n        input_shape = tensor_shape.TensorShape(input_shape)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:758 __init__\n        self._dims = [Dimension(d) for d in dims]\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:758 <listcomp>\n        self._dims = [Dimension(d) for d in dims]\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:203 __init__\n        six.raise_from(\n    <string>:3 raise_from\n        \n\n    TypeError: Dimension value must be integer or None or have an __index__ method, got value 'TensorShape([None, 1])' with type '<class 'tensorflow.python.framework.tensor_shape.TensorShape'>'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16704/4065291908.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompareAutoencoder_Average\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultsFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavNum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16704/1391158950.py\u001b[0m in \u001b[0;36mcompareAutoencoder_Average\u001b[1;34m(directory, model, latent_dim, epo, trainsize, x, y, testsize, avNum)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mLx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mLy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfileAutoencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkPSep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"greens\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkPSep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkPSep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"times\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkPSep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"posDiff\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkPSep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkPSep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16704/1267867660.py\u001b[0m in \u001b[0;36mfileAutoencode\u001b[1;34m(directory, model, latent_dim, epo, trainsize, testsize, x, y)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     autoencoder.fit((data_train),\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\maxgr\\AppData\\Local\\Temp/ipykernel_16704/1981640804.py:9 call  *\n        encoded = self.encoder(x)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:389 call\n        outputs = layer(inputs, **kwargs)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1008 __call__\n        self._maybe_build(inputs)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:1179 build\n        input_shape = tensor_shape.TensorShape(input_shape)\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:758 __init__\n        self._dims = [Dimension(d) for d in dims]\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:758 <listcomp>\n        self._dims = [Dimension(d) for d in dims]\n    C:\\Users\\maxgr\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:203 __init__\n        six.raise_from(\n    <string>:3 raise_from\n        \n\n    TypeError: Dimension value must be integer or None or have an __index__ method, got value 'TensorShape([None, 1])' with type '<class 'tensorflow.python.framework.tensor_shape.TensorShape'>'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compareAutoencoder_Average(resultsFolder, model2, latent_dim = 10, epo = 50, trainsize = 1, testsize = 1, avNum = 5, x = 0, y = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7d3b7-845f-460b-bd63-635d2db4947d",
   "metadata": {},
   "source": [
    "### DENOISE TIME DISPLACED GREENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ccbf6-aebb-47b8-9595-30e98a51b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbe491-8a68-4e2b-ad0e-b73fbe79620c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02887a49-7ce8-4280-9c47-6f8339664714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c94d60-1267-4e79-9eb0-96620247ebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31953799-8f43-4b05-910d-c6785f71f1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345c6f4-3463-48bd-97aa-30adde69c92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf74c4c-6e6b-4e51-a9ec-28d05cb4bd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c1f57-57dd-4708-86cc-d6b41455cad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bcd04-0425-4630-bd10-3afe72a8aec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f61c0-75eb-4281-9831-e73e01ab657d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
